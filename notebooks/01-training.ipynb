{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models with the Data-Driven Library\n",
    "\n",
    "The datadriven library provides an extensible command-line interface for training, evaluating, and predicting data-driven simulators. However, you may prefer training and sweeping models inside a notebook. This notebook provides an example for doing so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Working Directory and Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra.experimental import initialize, compose\n",
    "from omegaconf import DictConfig, ListConfig, OmegaConf\n",
    "from model_loader import available_models\n",
    "from base import plot_parallel_coords\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from rich import print\n",
    "from rich.logging import RichHandler\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(message)s\",\n",
    "    datefmt=\"[%X]\",\n",
    "    handlers=[RichHandler()]\n",
    ")\n",
    "logger = logging.getLogger(\"ddm_training\")\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Configuration\n",
    "\n",
    "While you can provide every argument manually, there is benefit in directly using the `hydra` config class to load an existing configuration file. This way you can ensure your parameters are saved to a file for later use, and you automatically gain the benefit of all the logging and model artifacts that are provided by our workflow of `hydra` and `mlflow`.\n",
    "\n",
    "If you want to override any settings of the configurations, provide them in a list of `overrides` as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize(config_path=\"../conf\", job_name=\"ddm_training\")\n",
    "cfg = compose(config_name=\"config\", overrides=[\"data=cartpole_st1_at\", \"model=torch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from yaml file\n",
    "input_cols = cfg['data']['inputs']\n",
    "output_cols = cfg['data']['outputs']\n",
    "augmented_cols = cfg['data']['augmented_cols']\n",
    "dataset_path = cfg['data']['path']\n",
    "iteration_order = cfg['data']['iteration_order']\n",
    "episode_col = cfg['data']['episode_col']\n",
    "iteration_col = cfg['data']['iteration_col']\n",
    "max_rows = cfg['data']['max_rows']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Trainer\n",
    "\n",
    "To make it easy to sweep over models later, we create a simple `train_models` function here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(config=cfg):\n",
    "\n",
    "    logger.info(f'Model type: {available_models[config[\"model\"][\"name\"]]}')\n",
    "    Model = available_models[config[\"model\"][\"name\"]]\n",
    "    global model\n",
    "    model = Model()\n",
    "    logger.info(f\"Loading data from {dataset_path}\")\n",
    "    global X, y\n",
    "    X, y = model.load_csv(\n",
    "        input_cols=input_cols,\n",
    "        output_cols=output_cols,\n",
    "        augm_cols=list(augmented_cols),\n",
    "        dataset_path=dataset_path,\n",
    "        iteration_order=iteration_order,\n",
    "        episode_col=episode_col,\n",
    "        iteration_col=iteration_col,\n",
    "        max_rows=max_rows,\n",
    "    )\n",
    "    logger.info(f\"Building model with parameters: {config}\")\n",
    "    model.build_model(\n",
    "        **config[\"model\"][\"build_params\"]\n",
    "    )\n",
    "\n",
    "\n",
    "    logger.info(f\"Fitting model...\")\n",
    "    model.fit(X, y)\n",
    "    logger.info(f\"Model trained!\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_models(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Sweeping\n",
    "\n",
    "The `datadrivenmodel` has an automatic solution for hyperparameter sweeping and tuning. These settings are provided in the config `model.sweep` parameters. Provide the limits of the variables you want to sweep over and the `sweep` method will automatically parallelize the sweep over the available number of cores and find the optimal solution according to your `scoring_func`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Parameters\n",
    "\n",
    "You can select the search algorithm you'd like to use: `bayesian` runs bayesian optimiziation (using scikit-optimize), `hyperopt` runs [Tree-Parzen Estimators](https://papers.nips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf) with the `hyperopt` package, `bohb` uses Bayesian Opt/HyperBand, or `optuna` which also runs Tree-Parzen estimators but using the [`optuna`](https://optuna.readthedocs.io/en/stable/) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(OmegaConf.to_yaml(cfg[\"model\"][\"sweep\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = OmegaConf.to_container(cfg[\"model\"][\"sweep\"][\"params\"])\n",
    "logger.info(f\"Sweeping with parameters: {params}\")\n",
    "\n",
    "sweep_df = model.sweep(\n",
    "    params=params,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    search_algorithm=cfg[\"model\"][\"sweep\"][\"search_algorithm\"],\n",
    "    num_trials=cfg[\"model\"][\"sweep\"][\"num_trials\"],\n",
    "    scoring_func=cfg[\"model\"][\"sweep\"][\"scoring_func\"],\n",
    "    results_csv_path=cfg[\"model\"][\"sweep\"][\"results_csv_path\"],\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Outputs\n",
    "\n",
    "All outputs are saved to a timestamped directory in `outputs`. This includes the model artifacts, hyperparameter tuning results, and a verbose log of the entire run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -lh outputs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree -L 3 outputs/2021-04-21/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Hyperparameter Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parallel_coords(sweep_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Saved Runs from CSV\n",
    "\n",
    "Runs are automatically saved to a CSV in the outputs directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_df2 = pd.read_csv(\"outputs/2021-04-21/10-29-25/xgboost_gridsearch/search_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parallel_coords(sweep_df2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
