{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f9415c5",
   "metadata": {},
   "source": [
    "# Training and Evaluating Models with the Data-Driven Library\n",
    "\n",
    "This notebook provides an overview of the tools built in the DDM for extracting predictions from your trained DDM and for evauating the performance of the DDM.\n",
    "\n",
    "---\n",
    "\n",
    "We utilize `hydra` to save the configuration of our datasets and our models. The default configuration is in the `conf/config.yaml` directory:\n",
    "\n",
    "```YAML\n",
    "defaults:\n",
    "  - data: house_energy.yaml\n",
    "  - model: xgboost.yaml\n",
    "  - simulator: house_energy_simulator.yaml\n",
    "```\n",
    "\n",
    "Note that the configuration file points to three additional configuration files for each component: the data, the model, and the simulator.\n",
    "\n",
    "While the configuration file already has default values specified you can override any element of the configuration file using the `overrides` option. For example, we can override the data configuration to instead use the `yaml` file specified in `data/cartpole-100K-cts.csv.yaml` and the model configuration to use the `yaml` file specified in `model/SVR.yaml`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59e9a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f30a1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra.experimental import initialize, compose\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from omegaconf import DictConfig, ListConfig, OmegaConf\n",
    "from model_loader import available_models\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from rich import print\n",
    "from rich.logging import RichHandler\n",
    "import copy\n",
    "from assessment_metrics_loader import available_metrics\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(message)s\",\n",
    "    datefmt=\"[%X]\",\n",
    "    handlers=[RichHandler()]\n",
    ")\n",
    "logger = logging.getLogger(\"ddm_notebook\")\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db9b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "GlobalHydra.instance().clear() \n",
    "initialize(config_path=\"../conf\", job_name=\"model_train_validate\")\n",
    "cfg = compose(config_name=\"config\", overrides=[\"data=house-energy\", \"model=xgboost\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effe6782",
   "metadata": {},
   "source": [
    "## 1. Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564d9e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from yaml file\n",
    "input_cols = cfg['data']['inputs']\n",
    "output_cols = cfg['data']['outputs']\n",
    "augmented_cols = cfg['data']['augmented_cols']\n",
    "dataset_path = cfg['data']['path']\n",
    "iteration_order = cfg['data']['iteration_order']\n",
    "episode_col = cfg['data']['episode_col']\n",
    "iteration_col = cfg['data']['iteration_col']\n",
    "max_rows = cfg['data']['max_rows']\n",
    "diff_state = cfg['data']['diff_state']\n",
    "test_perc = cfg['data']['test_perc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9105f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DATA STRUCTURE SELECTED:\")\n",
    "print(\" - input_cols:\", input_cols)\n",
    "print(\" - augmented_cols:\", augmented_cols)\n",
    "print(\" - output_cols:\", output_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c642d5",
   "metadata": {},
   "source": [
    "##  2. Model Definition\n",
    "\n",
    "The `available_models` dictionary provides wrappers for the available models in this repository. We utilize `cfg[\"model\"]` to load and build the model specified in the `model.yaml` file.\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Every model has its own hyperparameters, specified through the `cfg[\"model\"][\"build_params\"]` dictionary, which can be modified directly in the dictionary below or through the `hydra` overrides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ff4deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"model\"][\"build_params\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9fa5a5",
   "metadata": {},
   "source": [
    "## 3. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea7013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(config=cfg):\n",
    "\n",
    "    logger.info(f'Model type: {available_models[config[\"model\"][\"name\"]]}')\n",
    "    Model = available_models[config[\"model\"][\"name\"]]\n",
    "    model = Model()\n",
    "    logger.info(f\"Building model with parameters: {config}\")\n",
    "    model.build_model(\n",
    "        **config[\"model\"][\"build_params\"]\n",
    "    )\n",
    "    logger.info(f\"Loading data from {dataset_path}\")\n",
    "    X, y = model.load_csv(\n",
    "        input_cols=input_cols,\n",
    "        output_cols=output_cols,\n",
    "        augm_cols=list(augmented_cols),\n",
    "        dataset_path=dataset_path,\n",
    "        iteration_order=iteration_order,\n",
    "        episode_col=episode_col,\n",
    "        iteration_col=iteration_col,\n",
    "        max_rows=max_rows,\n",
    "    )\n",
    "    global X_train, y_train, episode_ids_train, X_test, y_test, episode_ids_test\n",
    "    train_id_end = int(np.floor(X.shape[0] * (1 - test_perc)))\n",
    "    X_train, y_train, episode_ids_train = (X[:train_id_end,],y[:train_id_end,],model.episode_ids[:train_id_end,])\n",
    "    X_test, y_test, episode_ids_test = (X[train_id_end:,],y[train_id_end:,],model.episode_ids[train_id_end:,])\n",
    "    \n",
    "    \n",
    "    logger.info(f\"Fitting model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    logger.info(f\"Model trained!\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2_score = available_metrics[\"r2_score\"]\n",
    "    logger.info(f\"R^2 score is {r2_score(y_test,y_pred)} for the test set.\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dec35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_models(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2268a8",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a05406b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(filename=cfg[\"model\"][\"saver\"][\"filename\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943b5f1a",
   "metadata": {},
   "source": [
    "### Data Structure of Saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a609e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Input_cols:  {model.features}\")\n",
    "logger.info(f\"Output_cols: {model.labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a45a37e",
   "metadata": {},
   "source": [
    "## 4. Model Evaluations\n",
    "\n",
    "We provide three methods for evaluating the errors of our trained models:\n",
    "\n",
    "1. Model predictive error: using a specified metric (such as R^2 or RMSE) and a test set, we evaluate the metric on the test set.\n",
    "2. Visualization of per-iteration predictions on a test set.\n",
    "3. Visualization of sequential predictions on a test set. Sequential prediction refers to feeding the predicted output back into the input over a full episode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a17c048",
   "metadata": {},
   "source": [
    "### 4.1. Overall Prediction Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115f7cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select your scoring method: r2_score, root_mean_squared_error, or mean_squared_error\n",
    "scoring_method = available_metrics[\"r2_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0111dfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test set\n",
    "per_iteration_eval_table = model.evaluate(X_test, y_test, scoring_method, marginal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cc9ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (per_iteration_eval_table[\"score\"] < 0.7).any():\n",
    "    logger.warn(\"Per-iteration assessment R^2 is low. Please review your model.\")\n",
    "\n",
    "per_iteration_eval_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54876a70",
   "metadata": {},
   "source": [
    "### 4.2. Per-Iteration Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77974bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the the input columns at time t to predict the output column(s) at time t+1\n",
    "y_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7822f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all prediction results\n",
    "label_count = np.shape(y_preds)[1]\n",
    "for i in range(label_count):\n",
    "    fig = plt.figure(figsize=(20,5))\n",
    "    plt.plot(y_test[:,i], \"green\")\n",
    "    plt.plot(y_preds[:,i], \"brown\", linestyle='--')\n",
    "    plt.title(f\"Per-iteration predictions: {model.labels[i]}\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.legend([\"Truth\", \"Prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e72d9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom in on a specific section\n",
    "iteration_start = -50\n",
    "iteration_stop = -1\n",
    "\n",
    "# Define which input column is the action\n",
    "action_col = \"action_command\"\n",
    "\n",
    "# Plot action changes -- zoomed in\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "action_idx = model.features.index(action_col)\n",
    "plt.title(f\"Plot [{iteration_start}:{iteration_stop}] actions (ensure action is not stale)\")\n",
    "plt.plot(X_test[iteration_start:iteration_stop,action_idx])\n",
    "plt.grid()\n",
    "    \n",
    "# Plot output changes -- zoomed in\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "label_idx = 0\n",
    "plt.title(f\"Plot [{iteration_start}:{iteration_stop}] state preds: {model.labels[label_idx]}\")\n",
    "plt.plot(y_test[iteration_start:iteration_stop,label_idx], \"green\")\n",
    "plt.plot(y_preds[iteration_start:iteration_stop,label_idx], \"brown\", linestyle='--')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.legend([\"Truth\", \"Prediction\"])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cd5ffe",
   "metadata": {},
   "source": [
    "### 4.3. Sequential Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98b209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the predicted output back into the input for a full episode.\n",
    "preds_sequentially = model.predict_sequentially(X_test, episode_ids=episode_ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcb9853",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d67cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sequential predictions for first test episode. \n",
    "# Do you notice any error propagation (i.e., predictions deviate more from truth over time)?\n",
    "episode_idx = episode_ids_test==episode_ids_test[-1]\n",
    "label_count = np.shape(y_preds)[1]\n",
    "for i in range(label_count):\n",
    "    fig = plt.figure(figsize=(20,5))\n",
    "    plt.plot(y_test[episode_idx,i], \"green\")\n",
    "    plt.plot(preds_sequentially[episode_idx,i], \"brown\", linestyle='--')\n",
    "    plt.title(f\"Per-iteration predictions: {model.labels[i]}\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.legend([\"Truth\", \"Prediction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e9b5ec",
   "metadata": {},
   "source": [
    "## 5. Comparing Model Evaluations\n",
    "\n",
    "If you want to compare various models, you can use the following section to save them in between runs.\n",
    "\n",
    "1. Select appropriate \"model_name\" tag, and run this section\n",
    "2. Change config through \"config.yaml\" (located at 'conf' folder)\n",
    "3. Rerun from Model Build (Steps 1-4), until this section\n",
    "4. Define a new value for \"model_name\" tag, and run this section again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc1552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select model name, and feats to extract\n",
    "model_name = \"xgb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec52587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# per-iteration score\n",
    "model_per_it_scores = copy.deepcopy(per_iteration_eval_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5383a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize models dictionary if it doesn't exist already\n",
    "if 'models_dict' not in locals():\n",
    "    models_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9678b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append tables to model using selected model name as key\n",
    "models_dict[model_name] = (model_per_it_scores,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61a97e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de2b9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine column names if needed\n",
    "for model_name, score_tables in models_dict.items():\n",
    "    for score_table in score_tables:\n",
    "        for col_name in score_table.columns:\n",
    "            if \"score\" in col_name and model_name not in col_name:\n",
    "                score_table.rename(columns = {col_name:model_name+\"_\"+col_name}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7affaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate across all models\n",
    "all_scores = None\n",
    "for model_name, score_tables in models_dict.items():\n",
    "    for score_table in score_tables:\n",
    "        if all_scores is None:\n",
    "            all_scores = score_table\n",
    "        else:\n",
    "            all_scores = all_scores.merge(score_table,how='outer')\n",
    "\n",
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82c237e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
